{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 1.9.0\n",
      "numpy version: 1.15.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "print('tensorflow version:', tf.__version__)\n",
    "print('numpy version:', np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_pts=10000, n_features=3, use_nonlinear=False, \n",
    "                    noise_std=0.1, train_test_split=4):\n",
    "    \"\"\"\n",
    "    n_pts - number of data points to generate\n",
    "    n_features - a positive integer - number of features\n",
    "    use_nonlinear - if True, generate non-linear data\n",
    "    \"\"\"\n",
    "    # Linear data or non-linear data?\n",
    "    if use_nonlinear:\n",
    "        weights = np.array([[1.0, 0.5, 0.2],[0.5, 0.3, 0.15]])\n",
    "    else:\n",
    "        weights = np.array([1.0, 0.5, 0.2])\n",
    "          \n",
    "    bias = np.ones(n_pts).reshape((-1,1))\n",
    "    low, high = -np.ones((n_pts,n_features),'float'), np.ones((n_pts,n_features),'float')\n",
    "    np.random.seed(42)\n",
    "    noise = np.random.normal(size=(n_pts, 1))\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    gen_X = np.random.uniform(low=low, high=high)\n",
    "    if use_nonlinear:\n",
    "        gen_y = (weights[0,0] * bias + np.dot(gen_X, weights[0, :]).reshape((-1,1)) + \n",
    "             np.dot(gen_X*gen_X, weights[1, :]).reshape([-1,1]) +\n",
    "             noise_std * noise)\n",
    "    else:\n",
    "        gen_y = (weights[0] * bias + np.dot(gen_X, weights[:]).reshape((-1,1)) + \n",
    "             noise_std * noise)\n",
    "    return gen_X, gen_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 3), (10000, 1))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear data with 3 features\n",
    "data_X, data_y = generate_data(n_pts=10000, n_features=3, use_nonlinear=False)\n",
    "data_X.shape, data_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7500, 3) (7500, 1) (2500, 3) (2500, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_X, data_y, test_size=0.25,random_state=714)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with Tensorflow\n",
    "#### 1. constant inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.997706  ],\n",
       "       [0.50064737],\n",
       "       [0.2005714 ],\n",
       "       [1.0004975 ]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add column of ones to training data\n",
    "X_ones = np.c_[X_train, np.ones((len(X_train), 1))]\n",
    "\n",
    "# define input constant values\n",
    "X = tf.constant(X_ones, dtype=tf.float32, name='X')\n",
    "y = tf.constant(y_train, dtype=tf.float32, name='y')\n",
    "\n",
    "# X transpose for normal equation\n",
    "XT = tf.transpose(X)\n",
    "\n",
    "# use normal equation to get model thetas\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()\n",
    "theta_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. placeholder with Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class LinRegress_NormalEq:\n",
    "    \"\"\"\n",
    "    class LinRegress_NormalEq - implements normal equation solution\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, learning_rate=0.05, L=0):\n",
    "        # input placeholders\n",
    "        self.X = tf.placeholder(tf.float32, [None, n_features], name=\"X\") \n",
    "        self.Y = tf.placeholder(tf.float32, [None, 1], name=\"Y\")\n",
    "    \n",
    "        # regression parameters for the analytical solution using the Normal equation\n",
    "        self.theta_in = tf.placeholder(tf.float32, [n_features+1,None])\n",
    "\n",
    "        # Adding a column of ones to the data matrix\n",
    "        data_plus_bias = tf.concat([tf.ones([tf.shape(self.X)[0], 1]), self.X], axis=1)\n",
    "        \n",
    "        # Normal Equation\n",
    "        XT = tf.transpose(data_plus_bias)\n",
    "        self.theta = tf.matmul(tf.matmul(\n",
    "            tf.matrix_inverse(tf.matmul(XT, data_plus_bias)), XT), self.Y)\n",
    "        \n",
    "        # mean square error in terms of theta = theta_in\n",
    "        self.lr_mse = tf.reduce_mean(tf.square(\n",
    "            tf.matmul(data_plus_bias, self.theta_in) - self.Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train MSE: 0.3300895\n",
      "test MSE: 0.34288436\n"
     ]
    }
   ],
   "source": [
    "model = LinRegress_NormalEq(n_features=X_train.shape[1])\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # get the model thetas\n",
    "    theta_values = sess.run(model.theta, feed_dict={\n",
    "        model.X: X_train, model.Y: y_train\n",
    "    })\n",
    "    # run the train data for MSE\n",
    "    lr_mse_train = sess.run(model.lr_mse, feed_dict = {\n",
    "        model.X: X_train, model.Y: y_train,\n",
    "        model.theta_in: theta_value\n",
    "    })\n",
    "    # run the test data for MSE\n",
    "    lr_mse_test = sess.run(model.lr_mse, feed_dict = {\n",
    "        model.X: X_test, model.Y: y_test, \n",
    "        model.theta_in: theta_value            \n",
    "    })\n",
    "\n",
    "print('train MSE:', lr_mse_train)\n",
    "print('test MSE:', lr_mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0004976 ],\n",
       "       [0.997706  ],\n",
       "       [0.50064737],\n",
       "       [0.20057149]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Placeholder with Maximum Likelihood Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinRegress_MLE:\n",
    "    \"\"\"\n",
    "    class LinRegress_MLE - implements Maximum likelihood Estimate (MLE)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, learning_rate=0.05, L=0):\n",
    "        import math as m\n",
    "        # input placeholders\n",
    "        self.X = tf.placeholder(tf.float32, [None, n_features], name=\"X\") \n",
    "        self.Y = tf.placeholder(tf.float32, [None, 1], name=\"Y\")\n",
    "    \n",
    "        # regression parameters for the analytical solution using the Normal equation\n",
    "        self.theta_in = tf.placeholder(tf.float32, [n_features+1,None])\n",
    "\n",
    "        # Adding a column of ones to the data matrix\n",
    "        data_plus_bias = tf.concat([tf.ones([tf.shape(self.X)[0], 1]), self.X], axis=1)\n",
    "        \n",
    "        # Estimate the model using the Maximum Likelihood Estimation (MLE)\n",
    "        # n_features+2: intercept and std of noise \n",
    "        self.weights = tf.Variable(tf.random_normal([n_features+2, 1]))\n",
    "        \n",
    "        # prediction from the model\n",
    "        self.output = tf.matmul(data_plus_bias, self.weights[:-1, :])\n",
    "\n",
    "        gauss = tf.distributions.Normal(loc=0.0, scale=1.0)\n",
    "        sigma = 0.0001 + tf.square(self.weights[-1]) \n",
    "        pi = tf.constant(m.pi)\n",
    "        log_LL = tf.log(0.00001 + (1/( tf.sqrt(2*pi)*sigma)) * gauss.prob((self.Y - self.output) / sigma ))  \n",
    "        \n",
    "        self.loss = - tf.reduce_mean(log_LL)\n",
    "        self.train_step = (tf.train.AdamOptimizer(learning_rate).minimize(self.loss), -self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.001132  ],\n",
       "       [0.99868095],\n",
       "       [0.50135475],\n",
       "       [0.20495257],\n",
       "       [0.31846252]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_iter = 100\n",
    "model = LinRegress_MLE(n_features=X_train.shape[1])\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        (_ , loss), weights = sess.run((model.train_step, model.weights), feed_dict={\n",
    "            model.X: X_train, model.Y: y_train\n",
    "            })\n",
    "    # after iteration, perform predictions\n",
    "    Y_test_predicted = sess.run(model.output, feed_dict={model.X: X_test})\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22869110739297585"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.metrics import mean_squared_error\n",
    "# mean_squared_error(y_true=y_test, y_pred=Y_test_predicted)\n",
    "\n",
    "# from sklearn.metrics import r2_score\n",
    "# r2_score(y_true=y_test, y_pred=Y_test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
