# About Entropy

<!-- markdownlint-disable MD033 -->

<style>
p { text-indent: 5%; }
li { margin-left: -15px; }
color1 { color: crimson; }
</style>

## Intuitions of Entropy

<color1>**Entropy is a measure of unpredictability of the state, or equivalently, of its average information content**.  

### Examples for Intuitions

[Entropy (information theory)](https://en.wikipedia.org/wiki/Entropy_(information_theory))

#### Political Polls

Usually, a political poll is conducted when election outcome is _unpredictable_, i.e. the support between two candidates is almost equal. In this case, **entropy is high, since the uncertainty of who gets elected is quite high**.  

- after first poll $\Rightarrow$
  - the poll gives some new _information_
  - i.e. entropy after first poll is reduced, comparing to before poll
- shortly after, a second poll $\Rightarrow$
  - the result can be almost predicted by first poll
  - also, the second will contain no new _information_
  - i.e. entropy does't change much

#### Coin Toss

- with Equal Probability
  - i.e. $p(head) = p(tail) = 0.5$
  - entropy is at its maximum (in this case), $H(Coin) = $
- with Unequal Probability, e.g. both sides are $head$
  - i.e. $p(head) = 1.0$, always results $head$
  - entropy at its minimum, $H(Coin) = 0$, i.e. there's "no uncertainty"

## Definitions of Entropy

### In dictionary

The word entropy finds its roots in the Greek **entropia**, which means _"a turning toward" or "transformation"_. The word was used **to describe the measurement of disorder** by the German physicist Rudolph Clausius and appeared in English in 1868. _A common example of entropy is that of ice melting in water. The resulting change from formed to free, from ordered to disordered increases the entropy_.  

[entropy](https://medium.com/@samuel.flender/entropy-d53e190f7d49)  

**Entropy can be considered as the amount of randomness, or equivalently, the absence of information.** Physically, entropy is a quantity that is proportional to the number of “microscopic configurations” that are consistent with the observed “macroscopic state”. In plain terms, this means that entropy measures the number of combinations in which you could re-arrange the atoms and molecules of a “thing” such as to observe the same structure of that thing.  

A way to think about entropy is in terms of _likelihood or probability_. Consider leaving a glass of water with an ice cube in a room and then checking back on it at a randomly chosen time between now and, say, a year from now. The probability that you are going to find the glass of water with the ice not melted at that random time is extremely small. This is another way to think about entropy: **The state with water plus ice is the state of lowest entropy because it is such an unlikely configuration in the overall timeline**.  

Some basic properties of Entropy:  

- entropy is denoted by $H$, $H(X) = - \sum_{i=1}^{n} \big( p_i \times \log_2(p_i) \big)$
- value of entropy always $\ge 0$
- if entropy $= 0$, the uncertainty/surprisal is at its lowest
  - e.g. a binary classification, all observations belong to one of the two classes
    - i.e. if predict next, it's always the one class
- if all events have _equal probability_, entropy is at its highest
  - High uncertainty and surprise
  - e.g. a binary classification, each class has 0.5 probability, entropy $= 1$
    - i.e. cannot predict next well, best guess is with 50% chance

[Entropy (information theory), wikipedia](https://en.wikipedia.org/wiki/Entropy_(information_theory))  

Entropy in information theory:  

- data communication system: source $\Rightarrow$ channel $\Rightarrow$ receiver
  - fundamental problem:
    - for receiver to identify what data was generated by the source
    - based on the signal it receives through the channel
- information entropy
  - an absolute limit on the shortest possible average length of a lossless compression encoding
    - of the data produced by a source
  - typically measured in bits (alternatively called "shannons")

When the data source has a lower-probability value (i.e., when a low-probability event occurs), the event carries more "information" ("surprisal") than when the source data has a higher-probability value. The amount of information conveyed by each event defined in this way becomes a random variable whose expected value is the information entropy.  

Entropy is zero when one outcome is certain to occur. The entropy quantifies these considerations when a probability distribution of the source data is known. The meaning of the events observed (the meaning of messages) does not matter in the definition of entropy. Entropy only takes into account the probability of observing a specific event, so the information it encapsulates is information about the underlying probability distribution, not the meaning of the events themselves.  

This implies that casting a die has more entropy than tossing a coin because each outcome of the die has smaller probability than each outcome of the coin.  

## Examples of Entropy

### Machine of Symbols (Letters)

[Information Theory part 12: Information Entropy (Claude Shannon's formula), youtube](https://www.youtube.com/watch?v=R4OlXb9aTvQ)  
[Information entropy, khanacademy](https://www.khanacademy.org/computing/computer-science/informationtheory/moderninfotheory/v/information-entropy)  

- Two machines: $M1$ and $M2$, both output symbols from $\{A, B, C, D \}$
  - $M1$ outputs the symbol randomly, i.e. each symbol has $probability = 0.25$
  - $M2$ outputs the symbol with varying probability
    - symbol $A$ has probability $p_A = 0.50$
    - symbol $B$ has probability $p_B = 0.125$
    - symbol $C$ has probability $p_C = 0.125$
    - symbol $D$ has probability $p_D = 0.25$
- Question: _For next symbol, on average how many questions asked to get the correct one?_
  - i.e. the average number of question asked to get the correct symbol?
  - i.e. the **expected number of questions**?

<p align='center'><img src='./references/figures/Entropy Example.svg'></p>

- For $Machine \; 1$
  - with equal probability, the **expected value is just 2**
    - i.e. on average, with 2 question asked, we can get the correct next symbol
    - or, $(0.25 \times 2) - (0.25 \times 2) - (0.25 \times 2) - (0.25 \times 2) = 2$
- For $Machine \; 2$
  - with different probability, the expected number differs for each symbol
    - symbol $A$, can be answered by just 1 question
    - symbol $B$ and symbol $C$, can be answered by 3 questions
    - symbol $C$, can be answered after 2 questions
  - expected value is computed by $E[X] = \sum_{i=1}^{k}x_i \cdot p_i$
    - i.e. probability of each symbol times number of questions needed
    - i.e. $(0.5 \times 1) - (0.125 \times 3) - (0.125 \times 3) - (0.25 \times 2) = 1.75$

$M1$ has entropy $2$, while $M2$ has entropy $1.75$. On average, we need fewer questions to find the correct symbol with $M2$. In terms of information, it's saying that **$M2$ has less information because there is _less uncertainty or suprise_ about its output** (less information needed to store the result, i.e. fewer bits needed). Claude Shannon calls this _measure of uncertainty_, the _entropy_.  

- entropy $H = \sum_{i=1}^{n} p_i \times (\text{number of questions})_i$
  - $= \sum_{i=1}^{n} p_i \times \log_2(\text{number of outcomes at the level})_i$
    - $log_2$ because we divide each question into 2 sub-questions
  - $= \sum_{i=1}^{n} p_i \times \log_2(\frac{1}{p_i}) = \sum_{i=1}^{n} p_i \times \log_2(p_i^{-1})$
    - $\frac{1}{p_i}$ to get the number of outcomes at the level
    - e.g. with $D$ and $p_D = 0.25$, if $D$ is 1 outcome, there're 3 others
    - e.g. with $B$ and $p_B = 0.125$, if $B$ is 1 outcome, there're 7 others
  - $= - \sum_{i=1}^{n} \big( p_i \times \log_2(p_i) \big)$

With equal probablity, entropy is at its maximum. Intuitively, _we can say that items have higher simlarity to each other thus more difficult to tell the difference_. With different probability, entropy starts to reduce and thus contains less uncertainty. Note that _having less information_ with entropy is actually better, since with information communication, that means less bits needs to be transmitted.  

## Applications of Entropy

## Resources

General Ideas:  

- [What is entropy? - Jeff Phillips, youtube](https://www.youtube.com/watch?v=YM-uykVfq_E)
